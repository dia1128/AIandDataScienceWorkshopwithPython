{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Intro to NLP Week 11 - Day 2 - Lecture Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnI4qsOubeiT"
      },
      "source": [
        "# Introduction to Deep NLP models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis. The tutorial is heavily based on [Tensorflow text classification](https://www.tensorflow.org/text/tutorials/text_classification_rnn) tutorial available online. We will explain the notebook in depth and discuss code as we go along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t78yU0oeSl6"
      },
      "source": [
        "We will import the required packages here. Note that we are importing `tfds` here. Tensorflow datasets package has a dataset called IMDB Reviews. We will use this dataset to train a deep learning model for text classification. Here, given a review of a movie, we will predict whether the movie review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We use Tensorflow Datasets package to load the data ****\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "# Step 1: Load and Preprocess the Dataset\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bec2e56-28eb-4797-e3c7-2295f4c0052d"
      },
      "source": [
        "# Define the dataset\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "# Get the train and test split from the dataset\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "# Print some details of the train_dataset\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteVA5R2C/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteVA5R2C/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteVA5R2C/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5_8ao2YOzXT",
        "outputId": "8e9e6933-45a4-4a6f-eb1f-8175e36517ac"
      },
      "source": [
        "info"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4_BGKyurao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b8df64-f327-43f7-bb97-cd6d7ffc63fb"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64 # hyper parameter"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "# .shuffle(BUFFER_SIZE) --*** we want to randomize the sequence or the order in which the model sees the input data.\n",
        "# .batch(BATCH_SIZE) # 64 -> amount of data that is sent at once for optimization.\n",
        "# .prefetch(tf.data.AUTOTUNE) --***\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Ru3xMl1_nHC0",
        "outputId": "a0b14d60-71e9-443d-a5e8-ae36f311ba2a"
      },
      "source": [
        "# this will give an error because we cannot index prefetch dataset\n",
        "\n",
        "train_dataset[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7597fae7feb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this will give an error because we cannot index prefetch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIa4hPPKpBom",
        "outputId": "82b344ec-853e-4ea6-f9d3-e8e64b0b925b"
      },
      "source": [
        "train_dataset.take(1)\n",
        "\n",
        "# we see that this data has two types of variables in it"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJC3eL3omjwe"
      },
      "source": [
        "# this is the raw text dataset. now we need to preprocess the data."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqkvdcFv41wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79031f8b-b800-476d-a350-6411accd156f"
      },
      "source": [
        "# we have to iterate through the train_dataset to print the values.\n",
        "examples_to_print = 2\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:examples_to_print])\n",
        "  print(type(example.numpy()[0]))\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:examples_to_print])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'A very strange and compelling movie. It\\'s about a very awkward and tightly wound man who attempts to navigate his life as a door-to-door fundraiser/salesman. The director was able to capture a very unnerving tone that really served the story well. Original and unsettling while also finding a great deal of humor in the pain that accompanies life. There is a sequence at a testing facility that really stood out and made me laugh out loud which is not something I do as frequently as I should. One of the more memorable films I\\'ve seen in a long while. Hasn\\'t left my mind and I look forward to future efforts by Bronstein. Fantastic performances all around. The simple line \"I really appreciate it.\" is now iconic to me.'\n",
            " b'All these reviewers are spot on. I\\'ve seen many bad films over the years, believe me, and this beats the lot!<br /><br />This is not just a \"so bad it\\'s good\" exploiter waste of time, but a genuine, hilarious, movie atrocity.<br /><br />CHECK OUT the white furry monster type thing!<br /><br />WET YOURSELF LAUGHING at Thom Christopher\\'s \"spell-weaving\" acting!<br /><br />GAPE IN SHEER A**E-CLENCHING DISBELIEF! at the threadbare sets!<br /><br />This is one of those \"European co-productions\". No wonder we have so many wars. I swear, some of the people taking part in \\'Wizards of the Lost Kingdom\\' aren\\'t actually aware they are appearing in a film!<br /><br />FACT! I originally watched this movie on HTV Wales late one night while suffering from concussion and sleep deprivation. I had to track down a copy several weeks later to make sure it was really this awful. It is. Worse even than Lee Majors in The Norseman, more laughable than all of John Derek\\'s films, this is, truly, the Citizen Kane of Trash.']\n",
            "<class 'bytes'>\n",
            "\n",
            "labels:  [1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmHDgjffmp39"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "### Create the text encoder in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "From [Tensorflow TextVectorization documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization):\n",
        "\n",
        "> This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one example = one string) into either a list of token indices (one example = 1D tensor of integer token indices) or a dense representation (one example = 1D tensor of float values representing data about the example's tokens).\n",
        "\n",
        "> If desired, the user can call this layer's adapt() method on a dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them. This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary.\n",
        "\n",
        "> The processing of each example contains the following steps:\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "2. Split each example into substrings (usually words)\n",
        "3. Recombine substrings into tokens (usually ngrams)\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "Now, let us create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temoa5tbk216"
      },
      "source": [
        "```python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None, standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace', ngrams=None, output_mode='int',\n",
        "    output_sequence_length=None, pad_to_max_tokens=False, vocabulary=None, **kwargs\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "source": [
        "# The maximum number of words that the vocabulary can have\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, # set the max number of words to be used in the Bag of Words.\n",
        "    standardize='lower_and_strip_punctuation', # standardization - lowercase the sentence and strip all puctuations.\n",
        "    split='whitespace', # tokenization - split the sentence based on whitespace.\n",
        "    ngrams=None, # Do not create any n-grams.\n",
        "    output_mode='int') # We need our output as a list of integers.\n",
        "\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text)) # adapt the train_dataset and run the text processing pipeline on it."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl-_U0EnmBn7"
      },
      "source": [
        "```python\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "\n",
        "samples_tr = train_dataset.take(1)\n",
        "[(example, label) for example, label in samples_tr]\n",
        "first_sentence = example[0].numpy()\n",
        "first_sentence\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "reivew = str(first_sentence)\n",
        "# lower case the corpus\n",
        "reivew = reivew.lower()\n",
        "\n",
        "# removing punctuations\n",
        "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "reivew = reivew.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# tokenize sentences to words\n",
        "tokenized_reivew = word_tokenize(reivew)\n",
        "print(\"Tokenized reivew: \", tokenized_reivew)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Short sentences are padded with a padding token in such a way that the length of the sentences are the same. Padding tokens doesn't affect the learning ability of the network as they are not considered towards the gradient updates. Any words in a new sentence that is not already in the vocabulary will be marked with an unknown token. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df19f3c4-2175-4dd3-eb68-c5202c1ebea7"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# '[UNK]' -> unknown token - to specify any words that are unknown to TensorFlow."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca1c95b-a541-48b0-ad72-d0fe5ee54103"
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  4,   1,  37, ...,   0,   0,   0],\n",
              "       [144, 287, 749, ...,   0,   0,   0],\n",
              "       [ 10, 888,  12, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVslGKEWo_Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca2a69a-a808-4c91-8a64-b76319cb7108"
      },
      "source": [
        "encoded_example.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 995)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70a-bKB88l8"
      },
      "source": [
        "# Step 2: Visualize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzaehj3YF59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6ee995-4b6a-4100-b773-14b25f37b67f"
      },
      "source": [
        "num_words = 15\n",
        "n = 0\n",
        "\n",
        "words_in_the_sentence = str(example[n].numpy()).split(' ')[:num_words] # slicing 15 words out of the first sentence in the dataset\n",
        "encodeded_id_of_the_words = encoded_example[n][:num_words] # slicing 15 words out of the encoded output of first sentence in the dataset\n",
        "\n",
        "print(\"Encoding\\tWord\")\n",
        "for word, encoded_id in zip(words_in_the_sentence, encodeded_id_of_the_words):\n",
        "  print(encoded_id, \"\\t\\t\", word)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding\tWord\n",
            "4 \t\t b'A\n",
            "1 \t\t priest\n",
            "37 \t\t who\n",
            "44 \t\t has\n",
            "1 \t\t abandoned\n",
            "25 \t\t his\n",
            "1 \t\t ministry\n",
            "879 \t\t meets\n",
            "4 \t\t a\n",
            "182 \t\t young\n",
            "133 \t\t man\n",
            "37 \t\t who\n",
            "44 \t\t has\n",
            "41 \t\t just\n",
            "75 \t\t been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNclBwnfXO_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c1ffd1-e146-44d6-95bf-d73d3daf3665"
      },
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'A priest who has abandoned his ministry meets a young man who has just been ordained.<br /><br />This movie is about the cruel dilemma between a life dedicated to God and faith and a life of more earthly pleasures. In post war France it is also about the mortal aspect of Faith itself.<br /><br />This may not be the movie of a lifetime but it is a sin to have allowed it to fall in oblivion. Besides, Pierre Fresnay is sublime.'\n",
            "Round-trip:  a [UNK] who has [UNK] his [UNK] meets a young man who has just been [UNK] br this movie is about the [UNK] [UNK] between a life [UNK] to god and [UNK] and a life of more [UNK] [UNK] in [UNK] war [UNK] it is also about the [UNK] [UNK] of [UNK] [UNK] br this may not be the movie of a [UNK] but it is a [UNK] to have [UNK] it to fall in [UNK] [UNK] [UNK] [UNK] is [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "\n",
            "Original:  b\"Those 2 points are dedicated the reasonable performance from Akshay Kumar. I know Bollywood films do not really strive to be realistic but PLEASE a Walt Disney production is more realistic than this plot. The father is dying and does what any good parent does...kick his son out the son with his PREGNANT wife. A few things that were too hard to swallow- 1. Priyanka 'cool indoor swimming pool in the bedroom' and to go from that to living hungry in her in-laws garden shed???????? 2. Akshay suddenly got the job as a stunt man, gets bitten by rabified dogs, to then just walk off. This film is an INSulT to our intelligence I really cant believe i contributed financially to the 'people' who made this film by taking my family to see it, we left the cinema with a frown, please do not subject yourself to this mess to watching this take my advice and do not waste your 'waqt'.\"\n",
            "Round-trip:  those 2 points are [UNK] the [UNK] performance from [UNK] [UNK] i know [UNK] films do not really [UNK] to be realistic but please a [UNK] disney production is more realistic than this plot the father is [UNK] and does what any good [UNK] [UNK] his son out the son with his [UNK] wife a few things that were too hard to [UNK] 1 [UNK] cool [UNK] [UNK] [UNK] in the [UNK] and to go from that to living [UNK] in her [UNK] [UNK] [UNK] 2 [UNK] [UNK] got the job as a [UNK] man gets [UNK] by [UNK] [UNK] to then just [UNK] off this film is an [UNK] to our [UNK] i really cant believe i [UNK] [UNK] to the people who made this film by taking my family to see it we left the cinema with a [UNK] please do not subject yourself to this mess to watching this take my [UNK] and do not waste your [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "\n",
            "Original:  b'I realize that living in the Western Plains of Wyoming during the 1900s was brutal, in fact, it probably is still brutal today, but was it monumental enough to transform into a seemingly \"made-for-TV\" movie? Also, women\\'s rights were still budding in this nation during this time, so to find an independent woman determined to start fresh in this harsh territory, and still show the realism of the era \\xc2\\x85 would it make for good viewing? Honestly, I don\\'t know. I have thought about this film for the past two days, and I still can\\'t seem to muster the strength to say that it was a horrible film, yet I can truthfully tell you that it wasn\\'t the greatest I have ever seen. From several hodgepodge styles of acting, to two mismatched actors playing devoid of emotion character, to some of the most gruesome PG rated scenes to ever come out of late 70s cinema, it is hard to fully get a good grasp on Heartland. Was it good? Was it bad? That may be up for you to view and decide yourself, but until then, here are moments I enjoyed and desperately hated! <br /><br />This film continues to be a struggle in my mind because there were some very interesting scenes. Scenes where I wasn\\'t sure what the director was doing or which direction he was headed, but somehow still seemed to work well as a whole. I thought the story as a whole was a very interesting, historical tale. I do not know much about living in Wyoming, especially during the early 1900s, so this film captured that image in my mind. The thought of very cold winters, no neighbors for miles upon miles, and this Polaroid-esquire view untouched by corporate America. It was refreshing to witness and sheer breathtaking to experience (though the television). There were scenes that really stood out in my mind, like the cattle-branding scene, the pig slaughtering scene, and the saddening homesteader that didn\\'t survive their journey, that just brought a true sense of realism to this story. Director Richard Pearce did a great job of bringing the view of Wyoming to the viewers, but I am not sure he brought decent players to accompany the view.<br /><br />While I will constantly compliment the scenery of this film, I had trouble coping with the actors that seemingly walked on the set and read their lines from cards on the side. Rip Torn seemed out of place in his role as Clyde Stewart, a loner that somehow finds a connection with Conchata Ferrell\\'s Elinore Randall. The two as actors have no chemistry at all. Their scenes that they share together are pointless and honestly void of any emotion. The pregnancy scene nearly had me in stitches because of the way these two \"veteran\" actors portrayed it. The brave Elinore does what she has to do to get the child out of her, while Clyde gives an approving nod when she is done. This is love? Was it supposed to be love? I don\\'t know, I think with stronger characters we would have seen a stronger bond, but with Torn and Ferrell, it felt like two actors just playing their parts. Other scenes that just seemed to struggle in my mind were ones like when the frozen horse \"knocks\" on the door for food or shelter, the constantly fading and growing compassion that Clyde had for Elinore\\'s daughter (I just didn\\'t believe it), the lack of true winter struggle, and the entire land scene. The land scene especially because I needed more explanation on what Elinore was doing, why she was doing it, and why Clyde would build her a house if they were married! It was these simple events that if taken the time to explore, would have made for a stronger film.<br /><br />Overall, I will go middle of the road with this feature. There were definitely elements that should have been explored deeper, such as the relationship between these two strangers and the ultimate homesteading goals of Elinore, but they were countered with some beautiful scenes of our nation. These panoramic scenes which, in the span of 100 years, have changes from vast mountains to enormous skyscrapers. While there were some brilliant scenes of realism (starring cattle and pigs), I just felt as if we needed more. Depth was a key element lacking in this film, which was overshadowed by marginal acting and a diminishing story. Pearce could have dove deeper into this untapped world, but instead left open loopholes and clich\\xc3\\xa9d Western characters. Ferrell carried her own, but Torn was completely miscast. Decent for a viewing, but will not be picked up again by me.<br /><br />Grade: ** out of *****'\n",
            "Round-trip:  i realize that living in the [UNK] [UNK] of [UNK] during the [UNK] was [UNK] in fact it probably is still [UNK] today but was it [UNK] enough to [UNK] into a [UNK] [UNK] movie also [UNK] [UNK] were still [UNK] in this [UNK] during this time so to find an [UNK] woman [UNK] to start [UNK] in this [UNK] [UNK] and still show the [UNK] of the [UNK] [UNK] would it make for good viewing [UNK] i dont know i have thought about this film for the past two days and i still cant seem to [UNK] the [UNK] to say that it was a horrible film yet i can [UNK] tell you that it wasnt the greatest i have ever seen from several [UNK] [UNK] of acting to two [UNK] actors playing [UNK] of [UNK] character to some of the most [UNK] [UNK] [UNK] scenes to ever come out of late 70s cinema it is hard to [UNK] get a good [UNK] on [UNK] was it good was it bad that may be up for you to view and [UNK] yourself but until then here are moments i enjoyed and [UNK] [UNK] br br this film [UNK] to be a [UNK] in my mind because there were some very interesting scenes scenes where i wasnt sure what the director was doing or which direction he was [UNK] but somehow still seemed to work well as a whole i thought the story as a whole was a very interesting [UNK] tale i do not know much about living in [UNK] especially during the early [UNK] so this film [UNK] that [UNK] in my mind the thought of very [UNK] [UNK] no [UNK] for [UNK] upon [UNK] and this [UNK] view [UNK] by [UNK] america it was [UNK] to [UNK] and [UNK] [UNK] to experience though the television there were scenes that really [UNK] out in my mind like the [UNK] scene the [UNK] [UNK] scene and the [UNK] [UNK] that didnt [UNK] their [UNK] that just brought a true sense of [UNK] to this story director richard [UNK] did a great job of [UNK] the view of [UNK] to the viewers but i am not sure he brought decent [UNK] to [UNK] the [UNK] br while i will [UNK] [UNK] the [UNK] of this film i had [UNK] [UNK] with the actors that [UNK] [UNK] on the set and read their lines from [UNK] on the side [UNK] [UNK] seemed out of place in his role as [UNK] [UNK] a [UNK] that somehow finds a [UNK] with [UNK] [UNK] [UNK] [UNK] the two as actors have no [UNK] at all their scenes that they [UNK] together are [UNK] and [UNK] [UNK] of any [UNK] the [UNK] scene nearly had me in [UNK] because of the way these two [UNK] actors portrayed it the [UNK] [UNK] does what she has to do to get the child out of her while [UNK] gives an [UNK] [UNK] when she is done this is love was it supposed to be love i dont know i think with [UNK] characters we would have seen a [UNK] [UNK] but with [UNK] and [UNK] it felt like two actors just playing their parts other scenes that just seemed to [UNK] in my mind were ones like when the [UNK] [UNK] [UNK] on the [UNK] for [UNK] or [UNK] the [UNK] [UNK] and [UNK] [UNK] that [UNK] had for [UNK] daughter i just didnt believe it the lack of true [UNK] [UNK] and the entire [UNK] scene the [UNK] scene especially because i needed more [UNK] on what [UNK] was doing why she was doing it and why [UNK] would [UNK] her a house if they were [UNK] it was these simple events that if taken the time to [UNK] would have made for a [UNK] filmbr br overall i will go middle of the [UNK] with this feature there were definitely elements that should have been [UNK] [UNK] such as the relationship between these two [UNK] and the [UNK] [UNK] [UNK] of [UNK] but they were [UNK] with some beautiful scenes of our [UNK] these [UNK] scenes which in the [UNK] of [UNK] years have [UNK] from [UNK] [UNK] to [UNK] [UNK] while there were some brilliant scenes of [UNK] [UNK] [UNK] and [UNK] i just felt as if we needed more [UNK] was a [UNK] [UNK] [UNK] in this film which was [UNK] by [UNK] acting and a [UNK] story [UNK] could have [UNK] [UNK] into this [UNK] world but instead left open [UNK] and [UNK] [UNK] characters [UNK] [UNK] her own but [UNK] was completely [UNK] decent for a viewing but will not be [UNK] up again by [UNK] br [UNK] out of                                                                                                                                                                                                              \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "# Step 3: Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()), # the size of vocab\n",
        "        output_dim=64, # the size of embeddings\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # Need an LSTM network with 64 nodes.\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1) # final classfication\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87a8-CwfKebw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ececd25-a50b-465d-b373-5ecbe66999f4"
      },
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41gw3KfWHus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808c04ce-d52d-4eae-e5c5-232909366462"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00015988]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgpuTeFNDzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a856e12d-ddfe-478f-ef28-ce1a854b7066"
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00015988]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "# Step 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw86wWS4YgR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f72d956-87cc-401d-9f16-988dea019f3d"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 94s 237ms/step - loss: 0.6168 - accuracy: 0.6060\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.4341 - accuracy: 0.8140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52iUqFJb7cZa"
      },
      "source": [
        "# Step 5: Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNbXi43YgUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c287ffc2-3a6d-4ec8-c345-e268ffbd7d47"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 53s 127ms/step - loss: 0.3759 - accuracy: 0.8362\n",
            "Test Loss: 0.3759354054927826\n",
            "Test Accuracy: 0.8361600041389465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjlVT4eH5Gf9",
        "outputId": "5c74adab-ae99-4d18-99e6-9597722b1ce2"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3616295]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80VjrOCk3GX"
      },
      "source": [
        "We can further improve the inference on new sentences by improving the way we present the predicted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6-giIqikvJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64cbe825-2182-447c-9474-951e96221bd6"
      },
      "source": [
        "# prediction is >= 0.0 --> positive review\n",
        "# else --> negative review\n",
        "\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "    \"This movie was bomb\", ## <---- \n",
        "    \"This movie was overrated\",\n",
        "    \"The film's unimaginative aesthetic language diminishes Murray and straitjackets her complexity.\",\n",
        "    \"This movie was clutch\",\n",
        "    \"This movie is awesome\"\n",
        "]\n",
        "\n",
        "predicted_scores = model.predict(np.array(inputs))\n",
        "predicted_labels = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Review: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review:  This is a fantastic movie.\n",
            "Predicted label:  Positive\n",
            "Review:  This is a bad movie.\n",
            "Predicted label:  Negative\n",
            "Review:  This movie was so bad that it was good.\n",
            "Predicted label:  Negative\n",
            "Review:  I will never say yes to watching this movie.\n",
            "Predicted label:  Negative\n",
            "Review:  Skip this movie.\n",
            "Predicted label:  Negative\n",
            "Review:  Don't waste your time.\n",
            "Predicted label:  Negative\n",
            "Review:  This movie was bomb\n",
            "Predicted label:  Negative\n",
            "Review:  This movie was overrated\n",
            "Predicted label:  Negative\n",
            "Review:  The film's unimaginative aesthetic language diminishes Murray and straitjackets her complexity.\n",
            "Predicted label:  Positive\n",
            "Review:  This movie was clutch\n",
            "Predicted label:  Negative\n",
            "Review:  This movie is awesome\n",
            "Predicted label:  Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Step 3 -> Assignment: Additional Architectures - Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "source": [
        "#step 3\n",
        "model_2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "\n",
        "    # Additional layer goes there.\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(..., return_sequences=False), # fill the number of LSTM units in place of ...\n",
        "\n",
        "    # if you want to add another layer to this network, remember to set return_sequences=True\n",
        "    # in the previous layer. We want the a multi-layer LSTM network to return the sequences to the\n",
        "    # following LSTM layer.\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "source": [
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "source": [
        "#step 4\n",
        "history_2 = model_2.fit(train_dataset, epochs=5,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "source": [
        "#step 5\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss_2)\n",
        "print('Test Accuracy:', test_acc_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions_2 = model_2.predict(np.array([sample_text]))\n",
        "print(predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history_2, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history_2, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeX7udCOgITx"
      },
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "]\n",
        "\n",
        "predicted_scores_2 = model_2.predict(inputs)\n",
        "predicted_labels_2 = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores_2]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels_2):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}