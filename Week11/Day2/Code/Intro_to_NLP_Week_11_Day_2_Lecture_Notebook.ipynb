{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Intro to NLP Week 11 - Day 2 - Lecture Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnI4qsOubeiT"
      },
      "source": [
        "# Introduction to Deep NLP models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis. The tutorial is heavily based on [Tensorflow text classification](https://www.tensorflow.org/text/tutorials/text_classification_rnn) tutorial available online. We will explain the notebook in depth and discuss code as we go along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t78yU0oeSl6"
      },
      "source": [
        "We will import the required packages here. Note that we are importing `tfds` here. Tensorflow datasets package has a dataset called IMDB Reviews. We will use this dataset to train a deep learning model for text classification. Here, given a review of a movie, we will predict whether the movie review is positive or negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# We use Tensorflow Datasets package to load the data ****\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "# Step 1: Load and Preprocess the Dataset\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1f2332-c3c2-40bd-9c4d-171bc410658c"
      },
      "source": [
        "# Define the dataset\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "# Get the train and test split from the dataset\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "# Print some details of the train_dataset\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd4_BGKyurao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b57253-e2aa-42da-9f1c-2d02b4f6dc83"
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64 # hyper parameter"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "# .shuffle(BUFFER_SIZE) --*** we want to randomize the sequence or the order in which the model sees the input data.\n",
        "# .batch(BATCH_SIZE) # 64 -> amount of data that is sent at once for optimization.\n",
        "# .prefetch(tf.data.AUTOTUNE) --***\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Ru3xMl1_nHC0",
        "outputId": "1d6927e1-5a2f-41d1-9e6b-316e4ad35800"
      },
      "source": [
        "# this will give an error because we cannot index prefetch dataset\n",
        "\n",
        "train_dataset[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a0be1c17c5b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIa4hPPKpBom",
        "outputId": "ce2386a7-3c62-4b09-dec8-226cd2f24100"
      },
      "source": [
        "train_dataset.take(1)\n",
        "\n",
        "# we see that this data has two types of variables in it"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJC3eL3omjwe"
      },
      "source": [
        "# this is the raw text dataset. now we need to preprocess the data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqkvdcFv41wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7df910-51e0-4ce8-802e-13951527ed24"
      },
      "source": [
        "# we have to iterate through the train_dataset to print the values.\n",
        "examples_to_print = 6\n",
        "\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:examples_to_print])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:examples_to_print])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'Superb cast, more please!<br /><br />If you can catch just about anything else written by Plater (or starring these wonderful actors). For anyone who doesn\\'t know Plater has a real feeling for jazz, my recommendation is to see the \\'Beiderbecke\\' trilogy whenever you can.<br /><br />\"There\\'s three kinds of jazz - Hot, Cool and \\'What time does the tune start?\\'\"'\n",
            " b\"I can't figure out how anyone can get a budget for a movie this bad. It's like the TV station are desperate for anything, anything at all. They're buried underneath a bunch of snow, the electricity constantly flashes on and off, yet magically there is a background light that stays constant. Where does all this (fake) light come from? That, and all that stupid bickering between the characters. They seem to be more interested in complaining to each other than trying to invent ways to survive. It tries to create that feel of emergency and people helping. But because it's such bad directing and acting, you will not your Florence Nightingale fix with this flick, sorry. I'm joining the negative feedback, and I concur that this is one of the worst movies ever.\"\n",
            " b\"Back in the day, I remembered seeing dumb Nintendo Power comics that had the same artwork as this show... and then word came up that this show was a coming to a television near me! I was not estatic, but curious... I was curious about how bad this show was gonna suck. My friends all said that this show had no real meanings and was too silly for straight people like me to enjoy (i'm actually gay), so I decided to watch the show with low expectations.<br /><br />WHAT A HORRIBLE EXPERIENCE!!!!!!!!!!! First off, I hate the new characters. Tiff and Tuff are so dumb and I hate how so many fanboys drool over Tiff, it's sad. I also hate how they made Chef Kalasaki (or whatever his nonstraight name was) a good guy who owned a restaurant. Bad move, 4Kids TV! Escargoon is nothing but a loser adviser to the King Dedede (who sucks big time in this show) and I hate the face of that one company that keeps supplying Dedede with those awful weapons to destroy Kirby. So stupid, I hate this show.<br /><br />I then began to hate Kirby even more since it was obvious Nintendo was just aching to get Kirby some popularity. Kirby'll never beat Mario in the fight for coolness, and Kirby will always be nothing but a tiny little cream puff of gayness. NUF SAID!!!\"\n",
            " b'The old axiom that bored people are boring people is well demonstrated in \"Women in Love.\" The script, taken from D. H. Lawrence\\'s novel, contains an endless flow of concepts that are, at best, sophomoric.<br /><br />What a pity so much effort went into so vacuous an exercise; what an empty array of characters given such attention. In spite of high production values, this film comes across as tedious as its personnel.<br /><br />A revisit in 2001 merely confirms a 1969 impression of juvenille minds in adult bodies, dawdling nowhere, and fumbling every step of the way.'\n",
            " b'I\\'ve read countless of posts about this game being so similar to Max Payne, when i played it the first time i thought it was a bit weird arcade-like game with a desire to rip-off the Max Payne style (not just bullet-time). So when i played it for a couple of hours i realized how much fun it is! and how different from \"Max Payne\", yeah the bullet time is a bit similar but i think it fits differently to the game-style. This game is non-stop action - a mix between a shoot\\'em up and a fight\\'em up, so much fun, as a big fan of Max Payne i must say that the storyline of DTR is not near to the greatness of Max Payne, the graphics are a bit average and some of the levels look the same, but if you want a bit more of that \"bullet-time\" you should definitely own this game.'\n",
            " b'This movie is not for everyone. You\\'re either bright enough to get \"it\" or you\\'re not. Fans of sci-fi films who don\\'t take themselves too seriously definitely will enjoy this movie. I recommend this movie for those who can appreciate spoofs and parodies. Everyone I\\'ve recommended this film to has enjoyed it. If you enjoy Monty Python or Mel Brooks films, you\\'ll probably enjoy this one. The voice characterizations are done in a tongue-in-cheek manner and the one-liners fly fast and furious.']\n",
            "\n",
            "labels:  [1 0 0 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmHDgjffmp39"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "### Create the text encoder in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "From [Tensorflow TextVectorization documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization):\n",
        "\n",
        "> This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one example = one string) into either a list of token indices (one example = 1D tensor of integer token indices) or a dense representation (one example = 1D tensor of float values representing data about the example's tokens).\n",
        "\n",
        "> If desired, the user can call this layer's adapt() method on a dataset. When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values, and create a 'vocabulary' from them. This vocabulary can have unlimited size or be capped, depending on the configuration options for this layer; if there are more unique values in the input than the maximum vocabulary size, the most frequent terms will be used to create the vocabulary.\n",
        "\n",
        "> The processing of each example contains the following steps:\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "2. Split each example into substrings (usually words)\n",
        "3. Recombine substrings into tokens (usually ngrams)\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "Now, let us create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Temoa5tbk216"
      },
      "source": [
        "```python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None, standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace', ngrams=None, output_mode='int',\n",
        "    output_sequence_length=None, pad_to_max_tokens=False, vocabulary=None, **kwargs\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "source": [
        "# The maximum number of words that the vocabulary can have\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE, # set the max number of words to be used in the Bag of Words.\n",
        "    standardize='lower_and_strip_punctuation', # standardization - lowercase the sentence and strip all puctuations.\n",
        "    split='whitespace', # tokenization - split the sentence based on whitespace.\n",
        "    ngrams=None, # Do not create any n-grams.\n",
        "    output_mode='int') # We need our output as a list of integers.\n",
        "\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text)) # adapt the train_dataset and run the text processing pipeline on it."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl-_U0EnmBn7"
      },
      "source": [
        "```python\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "\n",
        "samples_tr = train_dataset.take(1)\n",
        "[(example, label) for example, label in samples_tr]\n",
        "first_sentence = example[0].numpy()\n",
        "first_sentence\n",
        "\n",
        "# Python code to reproduce tensorflow TextVectorization\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "reivew = str(first_sentence)\n",
        "# lower case the corpus\n",
        "reivew = reivew.lower()\n",
        "\n",
        "# removing punctuations\n",
        "# !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "reivew = reivew.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# tokenize sentences to words\n",
        "tokenized_reivew = word_tokenize(reivew)\n",
        "print(\"Tokenized reivew: \", tokenized_reivew)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Short sentences are padded with a padding token in such a way that the length of the sentences are the same. Padding tokens doesn't affect the learning ability of the network as they are not considered towards the gradient updates. Any words in a new sentence that is not already in the vocabulary will be marked with an unknown token. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21cbc96-d45c-4971-898a-768d25e621af"
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# '[UNK]' -> unknown token - to specify any words that are unknown to TensorFlow."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21f2d1-d586-4b88-a68a-b643dcb505c6"
      },
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[895, 177,  52, ...,   0,   0,   0],\n",
              "       [ 10, 175, 811, ...,   0,   0,   0],\n",
              "       [143,   8,   2, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVslGKEWo_Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ce67c5-a2ba-4f61-dcac-f661b316cabc"
      },
      "source": [
        "encoded_example.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1007)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70a-bKB88l8"
      },
      "source": [
        "# Step 2: Visualize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYzaehj3YF59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a8f22c-f305-4a4c-ed04-b9186e41a26a"
      },
      "source": [
        "num_words = 15\n",
        "n = 0\n",
        "\n",
        "words_in_the_sentence = str(example[n].numpy()).split(' ')[:num_words] # slicing 15 words out of the first sentence in the dataset\n",
        "encodeded_id_of_the_words = encoded_example[n][:num_words] # slicing 15 words out of the encoded output of first sentence in the dataset\n",
        "\n",
        "print(\"Encoding\\tWord\")\n",
        "for word, encoded_id in zip(words_in_the_sentence, encodeded_id_of_the_words):\n",
        "  print(encoded_id, \"\\t\\t\", word)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding\tWord\n",
            "895 \t\t b'Superb\n",
            "177 \t\t cast,\n",
            "52 \t\t more\n",
            "1 \t\t please!<br\n",
            "13 \t\t /><br\n",
            "45 \t\t />If\n",
            "23 \t\t you\n",
            "69 \t\t can\n",
            "1 \t\t catch\n",
            "41 \t\t just\n",
            "43 \t\t about\n",
            "229 \t\t anything\n",
            "329 \t\t else\n",
            "420 \t\t written\n",
            "33 \t\t by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNclBwnfXO_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6275e975-7486-4fb7-bde7-47485a40a692"
      },
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'Superb cast, more please!<br /><br />If you can catch just about anything else written by Plater (or starring these wonderful actors). For anyone who doesn\\'t know Plater has a real feeling for jazz, my recommendation is to see the \\'Beiderbecke\\' trilogy whenever you can.<br /><br />\"There\\'s three kinds of jazz - Hot, Cool and \\'What time does the tune start?\\'\"'\n",
            "Round-trip:  superb cast more [UNK] br if you can [UNK] just about anything else written by [UNK] or [UNK] these wonderful actors for anyone who doesnt know [UNK] has a real feeling for [UNK] my [UNK] is to see the [UNK] [UNK] [UNK] you [UNK] br theres three [UNK] of [UNK] hot cool and what time does the [UNK] start                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "\n",
            "Original:  b\"I can't figure out how anyone can get a budget for a movie this bad. It's like the TV station are desperate for anything, anything at all. They're buried underneath a bunch of snow, the electricity constantly flashes on and off, yet magically there is a background light that stays constant. Where does all this (fake) light come from? That, and all that stupid bickering between the characters. They seem to be more interested in complaining to each other than trying to invent ways to survive. It tries to create that feel of emergency and people helping. But because it's such bad directing and acting, you will not your Florence Nightingale fix with this flick, sorry. I'm joining the negative feedback, and I concur that this is one of the worst movies ever.\"\n",
            "Round-trip:  i cant figure out how anyone can get a budget for a movie this bad its like the tv [UNK] are [UNK] for anything anything at all theyre [UNK] [UNK] a bunch of [UNK] the [UNK] [UNK] [UNK] on and off yet [UNK] there is a background light that [UNK] [UNK] where does all this [UNK] light come from that and all that stupid [UNK] between the characters they seem to be more interested in [UNK] to each other than trying to [UNK] ways to [UNK] it tries to create that feel of [UNK] and people [UNK] but because its such bad directing and acting you will not your [UNK] [UNK] [UNK] with this flick sorry im [UNK] the [UNK] [UNK] and i [UNK] that this is one of the worst movies ever                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "\n",
            "Original:  b\"Back in the day, I remembered seeing dumb Nintendo Power comics that had the same artwork as this show... and then word came up that this show was a coming to a television near me! I was not estatic, but curious... I was curious about how bad this show was gonna suck. My friends all said that this show had no real meanings and was too silly for straight people like me to enjoy (i'm actually gay), so I decided to watch the show with low expectations.<br /><br />WHAT A HORRIBLE EXPERIENCE!!!!!!!!!!! First off, I hate the new characters. Tiff and Tuff are so dumb and I hate how so many fanboys drool over Tiff, it's sad. I also hate how they made Chef Kalasaki (or whatever his nonstraight name was) a good guy who owned a restaurant. Bad move, 4Kids TV! Escargoon is nothing but a loser adviser to the King Dedede (who sucks big time in this show) and I hate the face of that one company that keeps supplying Dedede with those awful weapons to destroy Kirby. So stupid, I hate this show.<br /><br />I then began to hate Kirby even more since it was obvious Nintendo was just aching to get Kirby some popularity. Kirby'll never beat Mario in the fight for coolness, and Kirby will always be nothing but a tiny little cream puff of gayness. NUF SAID!!!\"\n",
            "Round-trip:  back in the day i [UNK] seeing dumb [UNK] power [UNK] that had the same [UNK] as this show and then word came up that this show was a coming to a television near me i was not [UNK] but [UNK] i was [UNK] about how bad this show was [UNK] [UNK] my friends all said that this show had no real [UNK] and was too silly for straight people like me to enjoy im actually [UNK] so i decided to watch the show with low [UNK] br what a horrible experience first off i hate the new characters [UNK] and [UNK] are so dumb and i hate how so many [UNK] [UNK] over [UNK] its sad i also hate how they made [UNK] [UNK] or whatever his [UNK] name was a good guy who [UNK] a [UNK] bad move [UNK] tv [UNK] is nothing but a [UNK] [UNK] to the king [UNK] who [UNK] big time in this show and i hate the face of that one [UNK] that keeps [UNK] [UNK] with those awful [UNK] to [UNK] [UNK] so stupid i hate this [UNK] br i then [UNK] to hate [UNK] even more since it was obvious [UNK] was just [UNK] to get [UNK] some [UNK] [UNK] never [UNK] [UNK] in the fight for [UNK] and [UNK] will always be nothing but a [UNK] little [UNK] [UNK] of [UNK] [UNK] said                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "# Step 3: Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87a8-CwfKebw"
      },
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41gw3KfWHus"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIgpuTeFNDzq"
      },
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "# Step 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "source": [
        "history = model.fit(train_dataset, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52iUqFJb7cZa"
      },
      "source": [
        "# Step 5: Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80VjrOCk3GX"
      },
      "source": [
        "We can further improve the inference on new sentences by improving the way we present the predicted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6-giIqikvJ9"
      },
      "source": [
        "# prediction is >= 0.0 --> positive review\n",
        "# else --> negative review\n",
        "\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "]\n",
        "\n",
        "predicted_scores = model.predict(np.array(inputs))\n",
        "predicted_labels = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Step 3 -> Assignment: Additional Architectures - Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "source": [
        "#step 3\n",
        "model_2 = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "\n",
        "    # Additional layer goes there.\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(..., return_sequences=False), # fill the number of LSTM units in place of ...\n",
        "\n",
        "    # if you want to add another layer to this network, remember to set return_sequences=True\n",
        "    # in the previous layer. We want the a multi-layer LSTM network to return the sequences to the\n",
        "    # following LSTM layer.\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "source": [
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "source": [
        "#step 4\n",
        "history_2 = model_2.fit(train_dataset, epochs=5,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "source": [
        "#step 5\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss_2)\n",
        "print('Test Accuracy:', test_acc_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions_2 = model_2.predict(np.array([sample_text]))\n",
        "print(predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history_2, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history_2, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeX7udCOgITx"
      },
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "    \"Skip this movie.\",\n",
        "    \"Don't waste your time.\",\n",
        "]\n",
        "\n",
        "predicted_scores_2 = model_2.predict(inputs)\n",
        "predicted_labels_2 = [\"Positive\" if x>=0.0 else \"Negative\" for x in predicted_scores_2]\n",
        "\n",
        "for input, label in zip(inputs, predicted_labels_2):\n",
        "  print(\"Question: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}